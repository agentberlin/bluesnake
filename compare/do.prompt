# BlueSnake vs ScreamingFrog Comparison Workflow

This workflow helps identify and fix discrepancies between BlueSnake and ScreamingFrog crawlers by running a comparison script and analyzing the results.

## Quick Start

### Generate a New Comparison Report

Always generate a fresh report before analysis:

```bash
uv run compare/compare_crawlers.py <domain>
```

Ask for the domain if not provided.

**Important**: Do NOT use the `--js-rendering` flag unless explicitly requested. The ScreamingFrog config (`rendering.seospiderconfig`) does not have JS rendering enabled, so BlueSnake should also run without JS rendering to ensure a fair comparison.

### Fast Validation (Skip ScreamingFrog)

When validating fixes without re-running ScreamingFrog (uses existing SF data):

```bash
uv run compare/compare_crawlers.py <domain> --bluesnake-only
```

**Note**: The BlueSnake server automatically restarts when you make changes (running with `air`), so you can validate fixes quickly.

## Analysis Approach

### Primary Analysis: JSON Diff File

**Start here for all investigations.** The comparison script generates a JSON diff file that compares:
- **BlueSnake data**: Retrieved via API from the running server
- **ScreamingFrog data**: Parsed from exported CSV files

**Location**: `/tmp/crawler_diff_{domain}_{timestamp}.json`

This file contains the structured differences between both crawlers. Use this as your primary source for identifying issues.

**Structure**:
```json
{
  "metadata": {
    "domain": "example.com",
    "timestamp": "2025-10-14T10:30:00",
    "bluesnake_crawl_id": "abc123",
    "screamingfrog_log": "/tmp/screamingfrog_example.com_20251014.log"
  },
  "url_diffs": {
    "missing_in_bluesnake_by_type": {
      "html": ["https://example.com/page1", ...],
      "css": ["https://example.com/style.css", ...],
      "javascript": ["https://example.com/app.js", ...],
      "image": ["https://example.com/logo.png", ...],
      "pdf": [...],
      "font": [...],
      "data": [...],
      "other": [...]
    },
    "only_in_bluesnake": ["https://example.com/extra-page", ...],
    "sf_by_type": {"html": 150, "css": 20, ...},
    "bs_by_type": {"html": 145, "css": 18, ...}
  },
  "status_diffs": [
    {
      "url": "https://example.com/page",
      "sf_status": 200,
      "bs_status": 404
    }
  ],
  "outlink_diffs": [
    {
      "url": "https://example.com/page1",
      "sf_count": 25,
      "bs_count": 20,
      "only_in_sf": ["https://example.com/link1", ...],
      "only_in_bs": ["https://example.com/link2", ...]
    }
  ],
  "page_attribute_diffs": {
    "depth": [{"url": "...", "sf": 1, "bs": 2}],
    "title": [{"url": "...", "sf": "Title A", "bs": "Title B"}],
    "h1": [{"url": "...", "sf": "H1 A", "bs": "H1 B"}],
    "word_count": [{"url": "...", "sf": 500, "bs": 450, "diff_pct": 10.0}],
    "indexable": [{"url": "...", "sf": "Indexable", "bs": "No, Noindex"}],
    "canonical": [{"url": "...", "sf": "https://...", "bs": "https://..."}]
  },
  "link_attribute_diffs": {
    "follow": [{"source": "...", "dest": "...", "sf": true, "bs": false}],
    "target": [{"source": "...", "dest": "...", "sf": "_blank", "bs": ""}],
    "rel": [{"source": "...", "dest": "...", "sf": "nofollow", "bs": ""}],
    "path_type": [{"source": "...", "dest": "...", "sf": "Absolute", "bs": "Root-Relative"}],
    "position": [{"source": "...", "dest": "...", "sf": "Navigation", "bs": "content"}],
    "link_type": [{"source": "...", "dest": "...", "sf": "Hyperlink", "bs": "anchor"}]
  }
}
```

### Secondary Analysis: Detailed Investigation

**Only use these when the JSON diff doesn't provide enough context:**

#### Option 1: BlueSnake API

Use the `bluesnake_crawl_id` to query the API directly:

- **Get crawl results**:
  ```
  http://localhost:8080/api/v1/crawls/{crawl_id}
  ```

- **Get page links**:
  ```
  http://localhost:8080/api/v1/crawls/{crawl_id}/pages/{url_encoded}/links
  ```

#### Option 2: ScreamingFrog Raw Files

**Location**: `/tmp/crawlertest/sf/`

**Key Files**:

1. **`internal_all.csv`** - All crawled URLs with metadata
   - Key columns: `Address`, `Status Code`, `Content Type`, `Title 1`, `Meta Description 1`, `H1-1`, `H2-1`, `Canonical Link Element 1`, `Word Count`, `Indexability`, `Crawl Depth`
   - Use to understand why SF found certain URLs and compare page attributes
   - Note: BlueSnake compares depth, title, H1, word count, indexability, and canonical

2. **`all_outlinks.csv`** - Link relationships between pages
   - Key columns: `Source`, `Destination`, `Anchor`, `Type`, `Follow`, `Target`, `Rel`, `Path Type`, `Link Position`
   - Shows which page links to which (Source â†’ Destination)
   - `Type`: "Hyperlink", "JavaScript", "Image", etc.
   - `Follow`: true/false (based on rel="nofollow", sponsored, ugc)
   - `Path Type`: "Absolute", "Root-Relative", "Relative"
   - Use to debug missing links and compare link attributes in BlueSnake

3. **`all_page_text.csv`** - Extracted text content from pages
   - Use if you need to analyze page content

4. **`all_page_source.csv`** - Raw HTML source of pages
   - Use if you need to inspect actual HTML

5. **ScreamingFrog Log**: `/tmp/screamingfrog_{domain}_{timestamp}.log`
   - Console output from ScreamingFrog execution
   - Useful for debugging SF crawl issues

**Reading Tips**:
- Read large CSV files in chunks using Read tool with `limit` and `offset` parameters
- Focus on patterns rather than individual URLs
- Use these files to investigate specific issues identified in the JSON diff

## Problem-Solving Process

1. **Run the comparison script** to generate a fresh JSON diff
2. **Analyze the JSON diff** to identify the most important discrepancy
3. **Research if needed**: Check how ScreamingFrog handles the issue (internet search, documentation)
4. **Investigate deeper** (only if JSON diff is insufficient):
   - Query BlueSnake API for specific page/link details
   - Examine ScreamingFrog CSV files for link extraction patterns
5. **Propose a fix**: If the fix changes a design decision or internal assumption, get approval first
6. **Implement and validate**: Use `--bluesnake-only` flag for quick validation

## Important Notes

- **Don't read** `compare_crawlers.py` unless necessary (you can trust it works)
- **Don't read** historical diff comparisons (too context-intensive)
- **Always generate** a new report instead of using past reports
- **Pick one** most important issue to fix at a time
- **Be smart** about reading large files - use chunks, focus on patterns
- Do whatever it takes to get to the fix
